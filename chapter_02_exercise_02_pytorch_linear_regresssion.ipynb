{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc173ad8207a2964",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise: Implementing a Simple Linear Regression Model with PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, we will be implementing a simple linear regression model using PyTorch, a popular open-source machine learning library. Linear regression is a fundamental algorithm in machine learning that is used to predict a target variable by fitting the best linear relationship between the dependent and independent variable.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Import the necessary libraries.\n",
    "2. Prepare the data: Create tensors for our input data (x) and output data (y). For simplicity, we will use a simple linear relationship y = 2x + 1.\n",
    "3. Initialize the parameters: Initialize our weight (w) and bias (b) using `torch.nn.Parameter`. `torch.nn.Parameter` is a kind of Tensor, that is automatically added to the list of its module’s parameters.\n",
    "4. Define the model: Our model is a simple linear model: y = wx + b.\n",
    "5. Define the loss function: We will use Mean Squared Error (MSE) as our loss function.\n",
    "6. Train the model: Train our model for 1000 epochs. In each epoch, perform the following steps:\n",
    "    - Forward pass: Compute predicted y by passing x to the model.\n",
    "    - Compute and print loss.\n",
    "    - Use autograd to compute the backward pass. This call will compute the gradient of loss with respect to all Tensors with `requires_grad=True`.\n",
    "    - Update weights using gradient descent. We need to use `torch.no_grad()` to prevent tracking history (and consuming memory).\n",
    "    - Manually zero the gradients after updating weights.\n",
    "7. Print out the final values of w and b.\n",
    "\n",
    "By the end of this exercise, you should be able to understand the basic workflow of creating and training a simple linear regression model with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949808819987618f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T04:07:17.161007200Z",
     "start_time": "2023-12-06T04:07:15.889838500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a41c0e6c261e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will create a tensor for our input data (x) and output data (y)\n",
    "# For simplicity, we will use a simple linear relationship y = 2x + 1\n",
    "x = torch.arange(-10, 10, 0.1).reshape(-1, 1)\n",
    "y = 2 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will initialize our weight (w) and bias (b) using torch.Parameter\n",
    "# torch.Parameter is a kind of Tensor, that is automatically added to the list of its module’s parameters\n",
    "# TODO: Initialize the weight and bias parameters (w and b)\n",
    "w =\n",
    "b ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model is a simple linear model: y = wx + b\n",
    "def model(x):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) cost function is defined as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ is the actual value\n",
    "- $\\hat{y_i}$ is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Mean Squared Error (MSE) as our loss function\n",
    "def loss_fn(y_pred, y_true):\n",
    "    # TODO: Compute the MSE loss\n",
    "    # Note: do not loop over the samples, use pytorch functions that can handle arrays instead\n",
    "    loss =\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot: Sure, here's the explanation with the formulas formatted using LaTeX:\n",
    "\n",
    "---\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in linear regression. It's used to find the best parameters (weights and bias) for our model.\n",
    "\n",
    "Given a cost function defined as the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - (wx^{(i)} + b))^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `n` is the number of observations\n",
    "- `y^(i)` is the actual value\n",
    "- `x^(i)` is the input value\n",
    "- `w` is the weight\n",
    "- `b` is the bias\n",
    "\n",
    "The goal of gradient descent is to find the values of `w` and `b` that minimize the cost function `J(w, b)`. \n",
    "\n",
    "The update rules for `w` and `b` are:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `α` is the learning rate\n",
    "- The partial derivatives represent the gradients of the cost function with respect to `w` and `b`.\n",
    "\n",
    "The gradients indicate the rate of change of the cost function at the current point, i.e., the slope of the cost function. The algorithm subtracts the gradients from the parameters because we want to move against the gradient (downhill) to minimize the cost function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the loss values for each epoch\n",
    "loss_values = []\n",
    "\n",
    "# We will train our model for 100 epochs\n",
    "for epoch in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    # TODO: Compute the predicted y using the model defined above\n",
    "    y_pred =\n",
    "\n",
    "    # Compute loss\n",
    "    # TODO: Compute the loss using the loss function defined above\n",
    "    loss =\n",
    "    \n",
    "    # Append the loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}: {loss.item()}')\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w.grad and b.grad will be Tensors holding the gradient of the loss with respect to w and b respectively\n",
    "    # TODO: Compute the gradients of w and b using autograd\n",
    "    \n",
    "\n",
    "    # Update weights using gradient descent. We need to use torch.no_grad() to prevent tracking history (and consuming memory)\n",
    "    with torch.inference_mode():\n",
    "        w -= w.grad * 0.01\n",
    "        b -= b.grad * 0.01\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # The reason for this is that .backward() accumulates the gradients on subsequent backward passes\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the learned parameters\n",
    "print(f'w = {w.item()}, b = {b.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, PyTorch provides a powerful and flexible framework for implementing models in machine learning. Its ability to perform automatic differentiation and compute gradients makes it particularly useful for linear regression, among other algorithms. The simplicity and intuitiveness of its API, along with its Pythonic nature, make PyTorch a great tool for both beginners and experts in machine learning. As we've seen in this exercise, with just a few lines of code, we can set up and train a linear regression model. This ease of use, combined with its robust features, makes PyTorch a leading tool in the field of deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
